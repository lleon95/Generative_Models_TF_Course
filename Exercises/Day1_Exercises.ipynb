{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. **ResNet**: Winner of ILSVRC 2015, the ResNet (Residual Network) managed to get way deeper then the previous networks. The key ingredient in ResNet is the residual block:\n",
    "![residual_block](https://www.oreilly.com/library/view/advanced-deep-learning/9781788629416/graphics/B08956_02_10.jpg)\n",
    "In this exercise you should implement a `Residual_Block` class by subclassing `Layer`. Then create a toy ResNet to train on CIFAR-10. The design of the network is up to you! My suggestion... Start with a couple of convolutional layers and a max pooling layer, then add 2 residual blocks and finish by flattening the tensor and a couple of dense layers.\n",
    "\n",
    "\n",
    "2. **Custom training**: Define a simple model to classify fashion_MNIST and write explicitly the training loop. At the end of each epoch compute the accuracy of the model on the validation set (you should split it in batches, run the predictions on each batch, and collect the results). Display the collected statistics on tensorboard.\n",
    "\n",
    "\n",
    "3. **Play around**: Check the [TensorFlow playground](http://playground.tensorflow.org/), it is a nice tool to have a visual representation of what is going on in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the residual block\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "class MyResBlock(layers.Layer):    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_size):\n",
    "        self.conv1 = layers.Conv2D(input_size[-1], 3, padding='same', use_bias=False)\n",
    "        self.bn1   = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.conv2 = layers.Conv2D(input_size[-1], 3, padding='same', use_bias=False)\n",
    "        self.bn2   = layers.BatchNormalization()\n",
    "        self.add   = layers.Add()\n",
    "        self.relu2 = layers.ReLU()\n",
    "        super().build(input_size)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        y = self.add([x, inputs])\n",
    "        return self.relu2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, BatchNormalization, Add, Layer\n",
    "from tensorflow.keras.layers import MaxPool2D, GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "def myResNet():\n",
    "    my_res = Sequential()\n",
    "    my_res.add(Conv2D(8, 3, activation='relu'))\n",
    "    my_res.add(Conv2D(16, 3, use_bias=False))\n",
    "    my_res.add(Conv2D(32, 3, use_bias=False))\n",
    "    my_res.add(BatchNormalization())\n",
    "    my_res.add(ReLU())\n",
    "    my_res.add(MaxPool2D(2))\n",
    "    my_res.add(MyResBlock())\n",
    "    my_res.add(Conv2D(64, 3, use_bias=False))\n",
    "    my_res.add(Conv2D(128, 3, use_bias=False))\n",
    "    my_res.add(BatchNormalization())\n",
    "    my_res.add(ReLU())\n",
    "    my_res.add(MaxPool2D(2))\n",
    "    my_res.add(MyResBlock())\n",
    "    my_res.add(Conv2D(256, 3, use_bias=False))\n",
    "    my_res.add(BatchNormalization())\n",
    "    my_res.add(ReLU())\n",
    "    my_res.add(GlobalAveragePooling2D())\n",
    "    my_res.add(Dropout(0.7))\n",
    "    my_res.add(Dense(64, activation='relu'))\n",
    "    my_res.add(Dropout(0.7))\n",
    "    my_res.add(Dense(10, activation='softmax'))\n",
    "    return my_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_ResNet():\n",
    "    my_res = myResNet()\n",
    "    my_res.compile(loss='sparse_categorical_crossentropy',\n",
    "                    optimizer=\"RMSProp\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return my_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Cifar10 Dataset\n",
    "import tensorflow as tf \n",
    "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train_full, x_test = x_train_full/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val,  y_train, y_val  = train_test_split(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  224       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  1152      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  128       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_res_block (MyResBlock)    multiple                  18688     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  18432     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            multiple                  73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_res_block_1 (MyResBlock)  multiple                  295936    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            multiple                  294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  1024      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  650       \n",
      "=================================================================\n",
      "Total params: 726,442\n",
      "Trainable params: 724,970\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the resnet\n",
    "model = myResNet()\n",
    "model.build(x_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 10\n",
    "batch_size = 16\n",
    "loss_fn    = tf.keras.losses.sparse_categorical_crossentropy\n",
    "optimizer  = tf.keras.optimizers.SGD()\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 2.1929 - sparse_categorical_accuracy: 0.1813 - val_loss: 1.8769 - val_sparse_categorical_accuracy: 0.3128\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.8990 - sparse_categorical_accuracy: 0.2722 - val_loss: 1.5909 - val_sparse_categorical_accuracy: 0.3894\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.7586 - sparse_categorical_accuracy: 0.3348 - val_loss: 1.4724 - val_sparse_categorical_accuracy: 0.4174\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.6516 - sparse_categorical_accuracy: 0.3803 - val_loss: 1.3570 - val_sparse_categorical_accuracy: 0.4924\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.5628 - sparse_categorical_accuracy: 0.4162 - val_loss: 1.3042 - val_sparse_categorical_accuracy: 0.5103\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.4963 - sparse_categorical_accuracy: 0.4428 - val_loss: 1.2397 - val_sparse_categorical_accuracy: 0.5398\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.4337 - sparse_categorical_accuracy: 0.4754 - val_loss: 1.2846 - val_sparse_categorical_accuracy: 0.5450\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.3739 - sparse_categorical_accuracy: 0.5013 - val_loss: 1.4533 - val_sparse_categorical_accuracy: 0.5198\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.3277 - sparse_categorical_accuracy: 0.5225 - val_loss: 1.0483 - val_sparse_categorical_accuracy: 0.6231\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 12s 5ms/step - loss: 1.2720 - sparse_categorical_accuracy: 0.5518 - val_loss: 1.2726 - val_sparse_categorical_accuracy: 0.5651\n",
      "10 epochs in 124.01s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "model = myResNet()\n",
    "\n",
    "model.compile(loss=loss_fn, optimizer=optimizer, metrics=[acc_metric])\n",
    "start = time()\n",
    "history = model.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1,\n",
    "                    validation_data=(x_val, y_val))\n",
    "stop = time()\n",
    "print(\"%d epochs in %.2fs\"%(epochs, stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Bring the Fashion_MNIST\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = np.float32(x_train/255.)\n",
    "x_test       = np.float32(x_test/255.)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor board\n",
    "from datetime import datetime\n",
    "import os\n",
    "logdir = \"logs/\"\n",
    "model_log_dir  = os.path.join(logdir, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "cb_tensorboard = tf.keras.callbacks.TensorBoard(log_dir=model_log_dir)\n",
    "\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training auxiliars\n",
    "def get_batch(batch_size):\n",
    "    idx = np.random.randint(low=0, high=len(x_train), size=batch_size)\n",
    "    return x_train[idx], y_train[idx]\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, loss_fn, optimizer, x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward\n",
    "        y_pred   = model(x_batch, training=True)\n",
    "        out_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        tot_loss = tf.add_n([out_loss] + model.losses)\n",
    "    # Backward    \n",
    "    gradients = tape.gradient(tot_loss, model.trainable_variables)\n",
    "    # Update\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return y_pred, tot_loss\n",
    "\n",
    "def predict(model, loss_fn, x_batch, y_batch):\n",
    "    y_pred   = model(x_batch, training=False)\n",
    "    out_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "    tot_loss = tf.add_n([out_loss] + model.losses)\n",
    "    return y_pred, tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_5 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 24, 24, 16)        1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 12, 12, 64)        1088      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                92170     \n",
      "=================================================================\n",
      "Total params: 94,554\n",
      "Trainable params: 94,522\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def myModel2():\n",
    "    my_res = Sequential()\n",
    "    my_res.add(layers.Reshape([28, 28, 1], input_shape=[28, 28]))\n",
    "    my_res.add(Conv2D(8, 3, activation='relu'))\n",
    "    my_res.add(Conv2D(16, 3, use_bias=False))\n",
    "    my_res.add(BatchNormalization())\n",
    "    my_res.add(ReLU())\n",
    "    my_res.add(MaxPool2D(2))\n",
    "    my_res.add(Dropout(0.7))\n",
    "    my_res.add(Dense(64, activation='relu'))\n",
    "    my_res.add(Dropout(0.7))\n",
    "    my_res.add(layers.Flatten())\n",
    "    my_res.add(Dense(10, activation='softmax'))\n",
    "    return my_res\n",
    "\n",
    "model = myModel2()\n",
    "model.build(x_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10: 12.3501s\tTrain: accuracy: 0.756 - last loss: 0.428\tValidation: accuracy 0.836 - mean loss 0.495\n",
      "Epoch 1/10: 12.1639s\tTrain: accuracy: 0.810 - last loss: 0.317\tValidation: accuracy 0.853 - mean loss 0.446\n",
      "Epoch 2/10: 12.1977s\tTrain: accuracy: 0.827 - last loss: 0.331\tValidation: accuracy 0.862 - mean loss 0.416\n",
      "Epoch 3/10: 12.1523s\tTrain: accuracy: 0.836 - last loss: 0.259\tValidation: accuracy 0.855 - mean loss 0.410\n",
      "Epoch 4/10: 12.1784s\tTrain: accuracy: 0.839 - last loss: 0.262\tValidation: accuracy 0.876 - mean loss 0.389\n",
      "Epoch 5/10: 12.1881s\tTrain: accuracy: 0.844 - last loss: 0.269\tValidation: accuracy 0.878 - mean loss 0.379\n",
      "Epoch 6/10: 12.1623s\tTrain: accuracy: 0.848 - last loss: 0.213\tValidation: accuracy 0.876 - mean loss 0.365\n",
      "Epoch 7/10: 12.1606s\tTrain: accuracy: 0.851 - last loss: 0.248\tValidation: accuracy 0.871 - mean loss 0.377\n",
      "Epoch 8/10: 12.1512s\tTrain: accuracy: 0.854 - last loss: 0.223\tValidation: accuracy 0.875 - mean loss 0.364\n",
      "Epoch 9/10: 12.1804s\tTrain: accuracy: 0.857 - last loss: 0.234\tValidation: accuracy 0.882 - mean loss 0.352\n",
      "10 epochs in 170.54s\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "mean_loss_metric = tf.keras.metrics.Mean()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "start_t = time()\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    # Train\n",
    "    for i in range(0,len(x_train),batch_size):\n",
    "        x_batch, y_batch = get_batch(batch_size)\n",
    "        y_pred, loss     = train_step(model, loss_fn, optimizer, x_batch, y_batch)\n",
    "        train_acc_metric.update_state(y_batch, y_pred)\n",
    "        mean_loss_metric.update_state(loss)\n",
    "    train_acc = train_acc_metric.result()\n",
    "    mean_loss = mean_loss_metric.result()\n",
    "    stop = time()\n",
    "    \n",
    "    # Register with Tensorboard\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar(\"mean_loss\", mean_loss, step=epoch)\n",
    "        tf.summary.scalar(\"train_accuracy\", train_acc, step=epoch)\n",
    "        \n",
    "    train_acc_metric.reset_states()\n",
    "    mean_loss_metric.reset_states()\n",
    "    \n",
    "    # Validate\n",
    "    for i in range(0,len(x_test),batch_size):\n",
    "        x_batch, y_batch = x_test[i:i+batch_size], y_test[i:i+batch_size]\n",
    "        y_pred, loss     = predict(model, loss_fn, x_batch, y_batch)\n",
    "        train_acc_metric.update_state(y_batch, y_pred)\n",
    "        mean_loss_metric.update_state(loss)\n",
    "    val_acc  = train_acc_metric.result()\n",
    "    loss_acc = mean_loss_metric.result()\n",
    "    train_acc_metric.reset_states()    \n",
    "    mean_loss_metric.reset_states()    \n",
    "    print(\"Epoch %d/%d: %.4fs\\tTrain: accuracy: %.3f - last loss: %.3f\\tValidation: accuracy %.3f - mean loss %.3f\"%\n",
    "          (epoch, epochs, stop - start, train_acc, loss, val_acc, loss_acc))\n",
    "stop_t = time()    \n",
    "print(\"%d epochs in %.2fs\"%(epochs, stop_t-start_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the results:\n",
    "\n",
    "![](img/day1_tb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 DL",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
